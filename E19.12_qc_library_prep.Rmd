---
title: "E19.12_qc_library_prep"
author: "Chandler Sutherland"
date: "2025-05-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
require(tidyverse)
library(openxlsx)
library(patchwork)
library(ggbeeswarm)
```

Copyright Â© Chandler Sutherland Email: [chandlersutherland\@berkeley.edu](mailto:chandlersutherland@berkeley.edu){.email}

Purpose: produce a custom QC report for the E19.12 experiment. 

## put back in yield info 

Relative to the amount of DNA input into the first part of the pipeline, only recover 0.4-1%. However, this is plenty of fmol to work with before PCR amplification. 


Look at the fragment distribution: 

Col-0 gDNA 7	CS26
Col-0 gDNA 9 	CS27
Col-0 gDNA 10 	CS28
msh2 gDNA 2	CS29
msh2 gDNA 4	CS30
msh2 gDNA 7	CS31
uni1d gDNA 1	CS32
uni1d gDNA 2	CS33
uni1d gDNA 3	CS34

```{r}
#load data 
frag_traces_1 <- read_csv("C:\\Users\\chand\\Box Sync\\Krasileva_Lab\\Research\\chandler\\Krasileva Lab\\E19\\fragment analysis\\20250514\\2025 05 13 17H 59M Electropherogram.csv") %>%
  pivot_longer(cols=`C1: CS 26`:`A9: CS 34`, names_to='sample', values_to='Intensity') %>% 
  separate('sample', c(NA, 'sample', 'number'))  %>% 
  rename(size = `Size (bp)`) %>% 
  mutate(sample_number=paste(sample, number, sep='_'))%>%
  mutate(plant_id=case_when(sample_number=='CS_26' ~ 'Col0_7', 
                            sample_number=='CS_27' ~ 'Col0_9',
                            sample_number=='CS_28' ~ 'Col0_10', 
                            sample_number=='CS_29' ~ 'msh2_2', 
                            sample_number=='CS_30' ~ 'msh2_4', 
                            sample_number=='CS_31' ~ 'msh2_7', 
                            sample_number=='CS_32' ~ 'uni1d_1', 
                            sample_number=='CS_33' ~ 'uni1d_2', 
                            sample_number=='CS_34' ~ 'uni1d_3'
                            ))%>%
  mutate(sample=case_when(sample_number=='CS_26' ~ 'sample_1', 
                            sample_number=='CS_27' ~ 'sample_2',
                            sample_number=='CS_28' ~ 'sample_3', 
                            sample_number=='CS_29' ~ 'sample_4', 
                            sample_number=='CS_30' ~ 'sample_5', 
                            sample_number=='CS_31' ~ 'sample_6', 
                            sample_number=='CS_32' ~ 'sample_7', 
                            sample_number=='CS_33' ~ 'sample_8', 
                            sample_number=='CS_34' ~ 'sample_9'
                            )) %>%
  separate(plant_id, sep='_', into=c('genotype', NA), remove=FALSE)

frag_traces_1$plant_id <- as.factor(frag_traces_1$plant_id)

pre_pcr <- ggplot() +
  geom_line(data=frag_traces_1, aes(x=size, y= Intensity, color=plant_id))+
  scale_x_log10(limits=c(10,NA), breaks=c(10, 100, 200, 300, 400,500, 600, 700, 800, 1000, 1500,3000,6000))+
  ylim(0, 2000)+
  theme_classic()+
  geom_vline(xintercept=200, linetype=2, color='grey')+
  geom_vline(xintercept=1000, linetype=2, color='grey')+
  facet_wrap(~genotype)

pre_pcr
```


Fragment Sizes:
```{r}
frag_traces_1 %>% filter(size > 100 & size <=1000) 
```


## Dilution and re-qPCR 

Measured pre-PCR libraries on 05/14 and 05/15 with a new set of standards. Used the positive control and previous standards to calibrate new standards (14.25pM standard 1). Diluted to ideally 0.01 nM on 05/19 and 05/20, re-qPCR checked, then back calculated with my dilution factor what the estimate of the library was. 

How much noise am I getting? 

```{r}
#read in conglomerate concentration data 
pre_pcr_1 <- read.xlsx("C:\\Users\\chand\\Box Sync\\Krasileva_Lab\\Research\\chandler\\Krasileva Lab\\E19\\Library qPCR\\E19.12 pre-pcr qpcr.xlsx", sheet='both_replicates')

pre_pcr_1$date <- factor(pre_pcr_1$date, levels=c('20250514', '20250515', '20250519', '20250520', '20250521', 'average'))

#start a common controls df 
controls <- pre_pcr_1 %>% filter(str_detect(library, "^positive"))

#summary statistics per day 
my_sum <- pre_pcr_1 %>%
  group_by(library, date) %>%
  summarise( 
    n=n(),
    mean=mean(estimate, na.rm=T),
    sd=sd(estimate, na.rm= T)
  ) %>%
  mutate( se=sd/sqrt(n))  %>%
  mutate( ic=se * qt((1-0.05)/2 + .5, n-1))

ggplot() +
  geom_bar(my_sum, mapping=aes(x=library, y=mean, fill=date), stat='identity', alpha=0.5, position='dodge') +
  geom_point(pre_pcr_1, mapping=aes(x=library, y=estimate, color=date, shape=replicate), position=position_dodge(width=1))+
  ggtitle("Library Estimate")+
  ylab('nM')+
  theme_classic()
```

The measurements from 20250514 and 15 are close-ish, but the estimates based on dilutions are greater, showing that there is some inaccuracy in measurement. 

However, serial dilutions are not true replicates. Recalculate based on actual new dilutions. 

```{r}
pre_pcr_1_avg <- read.xlsx("C:\\Users\\chand\\Box Sync\\Krasileva_Lab\\Research\\chandler\\Krasileva Lab\\E19\\Library qPCR\\E19.12 pre-pcr qpcr.xlsx", sheet='average')

pre_pcr_1_avg$date <- factor(pre_pcr_1_avg$date, levels=c('20250514', '20250515', '20250519', '20250520', '20250521', 'average'))

my_sum <- pre_pcr_1_avg %>%
  group_by(library) %>%
  summarise( 
    n=n(),
    mean=mean(estimate, na.rm=T),
    sd=sd(estimate, na.rm= T)
  ) %>%
  mutate( se=sd/sqrt(n))  %>%
  mutate( ic=se * qt((1-0.05)/2 + .5, n-1)) %>%
  mutate(cv= sd/mean*100)

my_sum
```

Very high CV when including the dilution calculated estimates.

Actual dilution results: 
```{r}
dilution_1 <- read.xlsx("C:\\Users\\chand\\Box Sync\\Krasileva_Lab\\Research\\chandler\\Krasileva Lab\\E19\\Library qPCR\\E19.12 pre-pcr qpcr.xlsx", sheet='dilution')

dilution_1$date <- factor(dilution_1$date, levels=c('20250519', '20250520', '20250521'))
nM <- ggplot(dilution_1)+
  geom_hline(yintercept=0.01, color='red')+
  geom_point(aes(x=library, y=estimate, color=date))+
  theme_classic()+
  ylim(0, 0.05)+
  xlab('')+
  ylab('working concentration (nM)')

fmol <- ggplot(dilution_1)+
  geom_hline(yintercept=0.2, color='red')+
  geom_hline(yintercept=0.18, color='red', linetype=2)+
  geom_hline(yintercept=0.22, color='red', linetype=2)+
  geom_point(aes(x=library, y=fmol, color=date))+
  ylim(0, 0.5)+
  theme_classic()+
  xlab('')+
  ylab('fmol input to PCR in 20ul')

nM + fmol + plot_layout(guides='collect')
```
Take 20250514 and 15 at face value, then compare "ideal" fmol of dilution based on these, then the actual fmol of the dilution. 
```{r}
#load everything and calculate ideal based only on the 20250514 and 15 estimates of library size. 
dilution_trials_1 <- pre_pcr_1 %>% 
  filter(date %in% c('20250515', '20250514')) %>% 
  group_by(library) %>% 
  summarize(estimate=mean(estimate, na.rm=T)) %>%
  mutate(vol_0519 = c(8.20, 3.88, 3.38, 2.96, 4.63, 4.10, 2.55, 1.98, 4.72, NA, NA)) %>%
  mutate(ideal_0519 = estimate*vol_0519/50) %>%
  mutate(est_0519 = c(0.023, 0.023, 0.025, 0.019, 0.028, 0.023, 0.009, 0.007, 0.015, NA, NA)) %>%
  mutate(vol_0520 = c(6.43, 2.96, 2.45, 2.45, 3.21, 3.17, 3.38, 2.94, 5.01, NA, NA))%>% 
  mutate(ideal_0520 = estimate*vol_0520/50) %>%
  mutate(est_0520 = c(0.013, 0.014, 0.012, 0.011, 0.012, 0.015, 0.011, 0.013, 0.011, NA, NA)) %>%
  mutate(vol_0521 = c(12.37, 5.13, 3.71, 5.02, 5.79, 4.82, 6.66, 5.37, 11.02, NA, NA))%>%
  mutate(ideal_0521 = estimate*vol_0521/100) %>%
  mutate(est_0521 = c(0.016, 0.014, 0.015, 0.019, 0.019, 0.016, 0.018, 0.016, 0.017, NA, NA))

clean_dil_trial <- dilution_trials_1 %>% 
  subset(select=c(library, ideal_0519, ideal_0520, ideal_0521, est_0519, est_0520, est_0521)) %>%
  filter(str_detect(library, '^library')) %>%
  pivot_longer(cols=!library, names_to='date', values_to='value') %>%
  separate(date, into=c('type', 'date'), sep='_') %>%
  mutate(date=paste0('2025',date)) %>%
  pivot_wider(names_from=type, values_from=value)

clean_dil_trial %>% mutate(factor=est/ideal) %>% group_by(date) %>% summarize(avg_conc=mean(est), avg_factor=mean(factor))

ggplot(clean_dil_trial)+
  geom_point(aes(x=ideal, y=est, color=date))+
  geom_abline(slope=1, intercept=0)+
  geom_hline(yintercept = 0.01911111, color='#F8766D')+
  geom_hline(yintercept=0.01244444, color='#00BA38')+
  geom_hline(yintercept=0.01666667, color='#619CFF')+
  #scale_x_log10()+
 # scale_y_log10()+
   ylim(0, 0.03)+
  xlim(0, 0.015)+
  theme_classic()
```
Ok, a lot more noise than my other graphs, so the estimates of the libraries from 20250514 and 20250515 are not super accurate. My best dilution series was 20250520, in that it was most consistently close to 0.010, but I'm like factoring in an underestimate. It's not actually that much closer to the ideal v estimate line. 

# Retry!
Did everything on the same day, with 6 independent dilutions per library. It's time to get tennis elbow. 

## Pre-pcr quantification 
```{r}
pre_pcr_2 <- read.xlsx("C:\\Users\\chand\\Box Sync\\Krasileva_Lab\\Research\\chandler\\Krasileva Lab\\E19\\Library qPCR\\20250527.xlsx", sheet='Final Result', startRow=2, cols=c(1:4)) %>%
  rename('library' = 'X1', 'replicate' = 'X2', 'estimate' = 'nM')
```

First, how well did this run go? Two metrics: positive controls and benchmarks, dilutions of previous libraries. 
```{r}
pos_control <- pre_pcr_2 %>% 
  filter(str_detect(library, '^positive')) %>% 
  mutate(date='20250527') %>% 
  subset(select=-fragment.size)

#make an actual clean positive control df 
all_controls <- rbind(pos_control, controls) %>% 
  mutate(replicate=str_replace(replicate, 'dilution', 'rep')) %>%
  mutate(library=str_trim(str_replace(library, 'positive control ', 'positive_'
                             )))%>%
  rbind(c('positive_old', 'rep 1', 0.23569239, 20250121)) %>%
  rbind(c('positive_old', 'rep 2', 0.308177149, 20250121)) %>%
  rbind(c('positive_old', 'rep 1', 0.296839313, 20250107)) %>%
  rbind(c('positive_old', 'rep 2', 0.291052034, 20250107)) %>%
  rbind(c('positive_old', 'rep 1', 0.212634199, 20250124)) %>%
  rbind(c('positive_old', 'rep 2', 0.290635638, 20250124)) %>%
  rbind(c('positive_old', 'rep 1', 0.197092129, 20241211)) %>%
  rbind(c('positive_old', 'rep 2', 0.212994431, 20241211)) %>%
  filter(date != 'average') %>%
  mutate(estimate=as.double(estimate))

all_controls %>% write_csv('pre_pcr_qPCR_positive_controls.csv')

ggplot(all_controls)+
  geom_boxplot(aes(x=library, y=estimate))+
  geom_beeswarm(aes(x=library, y=estimate, shape=replicate), color='darkgrey')+
  theme_classic()+
  geom_point(all_controls %>% filter(date=='20250527'), mapping=aes(x=library, y=estimate, shape=replicate), color='purple')
```

Within the realm of possibility. rep1 of positive old is a little low. Check out benchmarks 
```{r}
bench <- pre_pcr_2 %>% filter(str_detect(library, '^benchmark')) %>% mutate(date='20250527') %>% subset(select=c(library, estimate, date))

ideal <- data.frame(library = c('benchmark 1', 'benchmark 2', 'benchmark 3', 'benchmark 4', 'benchmark 5', 'benchmark 6'),
           estimate = c(0.1, 0.04, 0.02, 0.01, 0.005, 0.0095)) %>%
  mutate(date='ideal')

bench_comp <- data.frame(library = c('benchmark 1', 'benchmark 2', 'benchmark 3', 'benchmark 4', 'benchmark 5', 'benchmark 6'),
           estimate = c(0.10236388, 0.056525321, 0.035209386, 0.012646095, 0.00784119, NA)) %>%
  mutate(date='20250121') %>%
  rbind(bench) %>%
  rbind(ideal)

ggplot(bench_comp)+
  geom_point(aes(x=library, y=estimate, color=date))+
  ylab('nM estimate')+
  theme_classic()+
ggplot(bench_comp)+
  geom_point(aes(x=library, y=estimate, color=date))+
  scale_y_log10()+
  ylab('log10(nM estimate)')+
  theme_classic()+
  plot_layout(guides='collect')
```
Pretty close. Except as seen previously, have a higher time with larger estimates. But otherwise ok, proportional to size of library. 

```{r}
lib_only <- pre_pcr_2 %>% filter(str_detect(library, '^Library'))

ggplot(lib_only)+
  geom_boxplot(aes(x=library, y=estimate))+
  geom_point(aes(x=library, y=estimate, color=replicate))+
  theme_classic()+
  xlab('')+
  ylab('working concentration (nM)')

noise_summary <- lib_only %>% group_by(library) %>%
  summarise( 
    n=n(),
    mean=mean(estimate, na.rm=T),
    sd=sd(estimate, na.rm= T)
  ) %>%
  mutate( se=sd/sqrt(n))  %>%
  mutate( ic=se * qt((1-0.05)/2 + .5, n-1)) %>%
  mutate(cv = sd/mean*100)

ggplot() +
  geom_bar(noise_summary, mapping=aes(x=library, y=mean), stat='identity', alpha=0.5) +
  geom_errorbar(noise_summary, mapping=aes(x=library, ymin=mean-ic, ymax=mean+ic), width=0.4, alpha=0.9, position='dodge') +
  geom_point(lib_only, mapping=aes(x=library, y=estimate))+
  ggtitle("Library Estimate (confidence intervals)")+
  ylab('nM')+
  theme_classic()

noise_summary
```
Pretty tight, some issue with dilution 4 library 3. Library 9 is a lot less than the others. Coefficient of variation between different measurements of the same library between 10-22% (except for Library 3). This is acceptable, and an improvement on last time. 


```{r}
#ng/ul measurement
noise_summary %>% subset(select=c(library, mean)) %>%
  mutate(ng_ul=mean/10^6*660*517) 
```

## Dilution and re-measurement 

In an attempt to prevent super crazy dilutions over and over, did three dilutions for each library. Tube 1 should be 0.0075, tube 2 0.01, tube 3 0.00125 
```{r}
dil_2 <- read.xlsx("C:\\Users\\chand\\Box Sync\\Krasileva_Lab\\Research\\chandler\\Krasileva Lab\\E19\\Library qPCR\\20250527_2.xlsx", sheet='Final Result', startRow=2, cols=c(1:5)) %>%
  rename('library' = 'X1', 'replicate' = 'X3', 'estimate' = 'nM')

dil_2
```

Once again, check controls 
```{r}
pos_control <- dil_2 %>% filter(str_detect(library, '^positive')) %>% mutate(date='20250527_2') %>% subset(select=-c(fragment.size, Tube))

all_controls <- rbind(pos_control, all_controls) %>% 
  mutate(replicate=str_replace(replicate, 'dilution', 'rep')) %>%
  mutate(library=str_trim(str_replace(library, 'positive control ', 'positive_'
                             )))

all_controls %>% write_csv('pre_pcr_qPCR_positive_controls.csv')

ggplot(all_controls)+
  geom_boxplot(aes(x=library, y=estimate))+
  geom_point(aes(x=library, y=estimate, color=date, shape=replicate), position='jitter')+
  scale_color_manual(values=c("20250527_2" = 'purple', "20250527" = 'lavender'))+
  #scale_y_log10()+
  theme_classic()
```
Acceptable!

Is there a significant relationship between time and factor difference between ideal and estimate? Caveat: 20250527 replicates were done on the same dilutions, so I should probably average them 
```{r}
bench_2 <- dil_2 %>% filter(str_detect(library, '^benchmark')) %>% 
  mutate(date='20250527_2') %>% 
  subset(select=c(library, estimate, date))

bench_comp_2 <- bench_comp %>% rbind(bench_2)

ggplot(bench_comp_2)+
  geom_point(aes(x=library, y=estimate, color=date))+
  scale_y_log10()+
  ylab('log10(nM)')+
  theme_classic()

versus <- bench_comp_2 %>% 
  pivot_wider(names_from=date, values_from=estimate) %>% 
  pivot_longer(cols = starts_with('2025'), names_to='date', values_to = 'estimate') %>%
  mutate(estimate=case_when(date=='20250121' & library=='benchmark 6' ~ NA, 
                            .default=estimate)) %>%
  mutate(date=case_when(date=='20250527_2' ~ '20250527', 
                        .default=date))%>%
  group_by(date, library, ideal)%>%
  summarize(estimate=mean(estimate, na.rm=T))

ggplot(versus)+
  geom_point(aes(x=ideal, y=estimate, color=date))+
  geom_abline(slope=1, intercept=0)+
  ylim(0, 0.18)+
  xlim(0, 0.18)+
  scale_x_log10()+
  scale_y_log10()+
  theme_classic()+
  labs(title='expected vs estimated dilution concentration')

factor_dif <- versus %>% mutate(dif=estimate-ideal) %>% mutate(factor=estimate/ideal) 

factor_dif %>%
  ggplot(aes(x=date, y=factor))+
  geom_hline(yintercept=1)+
  ylab('factor difference between ideal and actual dilution nM')+
  ylim(0, 2.5)+
  geom_boxplot()+
  geom_point(aes(color=library))+
  theme_classic()

factor_dif %>% group_by(date) %>% summarize(mean(factor, na.rm=T))

x <- factor_dif %>% filter(date=='20250121') %>% pull(factor)
y <- factor_dif %>% filter(date=='20250527') %>% pull(factor)

t.test(x, y)
```

There was always overestimation of libraries, and my new standards do not increase this significantly (1.4x to 1.6x).

In an attempt to prevent super crazy dilutions over and over, did three dilutions for each library. Tube 1 should be 0.0075, tube 2 0.01, tube 3 0.00125 
```{r}
dilution <- dil_2 %>% filter(str_detect(library, '^Library'))

nM <- ggplot(dilution)+
  geom_hline(yintercept=0.01, color='#00BA38')+
  geom_hline(yintercept=0.0075, color='#F8766D')+
  geom_hline(yintercept=0.0125, color='#619CFF')+
  geom_boxplot(aes(x=library, y=estimate, color=Tube))+
  theme_classic()+
  ylim(0, 0.03)+
  xlab('')+
  ylab('working concentration (nM)')

summary <- dilution %>% group_by(library, Tube) %>% summarize(estimate=mean(estimate, na.rm=T)) %>% mutate(fmol=estimate*20)

fmol <- ggplot(summary)+
  geom_hline(yintercept=0.2, color='red')+
  geom_hline(yintercept=0.18, color='red', linetype=2)+ #lower bound conservative estimate
  geom_hline(yintercept=0.22, color='red', linetype=2)+ #upper bound empirical estimate 
  geom_point(aes(x=library, y=fmol, color=Tube))+
  ylim(0, 0.5)+
  theme_classic()+
  xlab('')+
  ylab('fmol input to PCR in 20ul')

nM + fmol + plot_layout(guides='collect')
```

```{r}
dilution %>% 
  group_by(Tube) %>% 
  summarize(mean=mean(estimate, na.rm=T)) %>% 
  mutate(ideal=c(0.0075, 0.01, 0.0125)) %>% 
  mutate(dif=mean-ideal) %>%
  mutate(factor=mean/ideal)%>%
  mutate(fmol=mean*20)

dilution %>% filter(Tube=='tube 1') %>% group_by(library) %>% summarize(nM=mean(estimate))
```

Proceed with tube 1. 

The issue isn't inaccurate pipetting, it seems to be that no matter what there's an issue where I make the wrong estimate of the first library, leading to a inflated after dilution. This is probably because my qPCR based measurement isn't accurate, as in the standard is more accurate at certain concentrations. In my first library, I messed up my initial math and that likely compensated for this. I don't really know how to proceed with this. I wonder if that's why my standards always have messed up delta Ct values at the end. But everything I measure is in the same ballpark anyway. 

Comparing benchmarks to the dilutions: 
```{r}
more_versus <- dilution %>% 
  group_by(library, Tube) %>% 
  summarize(estimate=mean(estimate, na.rm=T)) %>% 
  mutate(ideal = case_when(Tube == 'tube 1' ~ 0.0075, 
                                      Tube == 'tube 2' ~ 0.01, 
                                      Tube == 'tube 3' ~ 0.0125))

ggplot(more_versus)+
  geom_point(aes(x=ideal, y=estimate, color=Tube))+
  geom_abline(slope=1, intercept=0)+
  ylim(0, 0.02)+
  xlim(0, 0.02)+
  theme_classic()

bench_and_dil <- more_versus %>% 
  mutate(date='20250527_2') %>% 
  rbind(versus %>% mutate(Tube=library)) 

bench_and_dil %>%
  ggplot()+
  geom_point(aes(x=ideal, y=estimate, color=Tube))+
  geom_abline(slope=1, intercept=0)+
  geom_hline(yintercept=0.01, color='red')+
  geom_hline(yintercept=0.012, color='red', linetype=2)+
  #ylim(0, 0.02)+
  #xlim(0, 0.02)+
  scale_x_log10(guide='axis_logticks')+
  scale_y_log10()+
  theme_classic()

bench_and_dil %>%
  ggplot()+
  geom_point(aes(x=library, y=estimate, color=Tube, shape=date))+
  theme_classic()+
  ylim(0, 0.02)

dil_2 %>% mutate(date='20250527')

```

# Library amplification efficiency 
WW suggested that it was a difference in primer efficiency between my standard curve and sample. Test with benchmark 20250527 and original results. 

Import a random standard curve, how about from 20250527 
```{r}
example_sc <- read.xlsx("C:\\Users\\chand\\Box Sync\\Krasileva_Lab\\Research\\chandler\\Krasileva Lab\\E19\\Library qPCR\\20250527.xlsx", sheet='Standard Curve', startRow=2, fillMergedCells = TRUE)  %>%
  mutate(CT=as.double(CT)) %>%
  rename('Ct' = 'CT', 
         'library' = 'X5')

example_sc %>% 
  subset(select=c(library, Concentration.pM, Average.CT)) %>% 
  unique() %>%
  mutate(log_pM = log10(Concentration.pM))
```



```{r}
#start with january 2021 measurement 
raw_1 <- read.xlsx("C:\\Users\\chand\\Box Sync\\Krasileva_Lab\\Research\\chandler\\Krasileva Lab\\E19\\Library qPCR\\20250121 dilution curve quant 1(SYBR) recalc 20250529.xlsx", sheet='Concentration Analysis', startRow=1, fillMergedCells = TRUE)  %>%
  mutate(Ct=as.double(Ct))

benchmark_raw_1 <- raw_1 %>% 
  filter(str_detect(sample, '^Dilution')) %>% 
  subset(select=c(sample, Average.Ct, dilution.factor)) %>% 
  unique() %>% 
  separate(sample, into=c(NA, 'dilution', 'replicate'), sep=' ') %>%
  mutate(benchmark = case_when(dilution == '2' ~ '1', 
                               dilution == '3' ~ '2', 
                               dilution == '4' ~ '3', 
                               dilution == '5' ~ '4', 
                               dilution == '6' ~ '5')) %>%
  mutate(replicate = case_when(replicate == '1:100' ~ '1', 
                               replicate == '1:10,000' ~ '2'))%>% 
  mutate(ideal_nM = case_when(benchmark == '1' ~ 0.1, 
                              benchmark == '2' ~ 0.04, 
                              benchmark == '3' ~ 0.02, 
                              benchmark == '4' ~ 0.01, 
                              benchmark == '5' ~ 0.005, 
                              benchmark == '6' ~ 0.0095)) %>% 
  mutate(ideal_nM=case_when(dilution == '7' ~ 0.0025, 
                            .default = ideal_nM))%>%
  mutate(log_pM = log10(ideal_nM/dilution.factor*1000))

model_20250121 <- lm(Average.Ct~log_pM, benchmark_raw_1)

text_formula <- paste0("y = ", as.character(round(model_20250121[[1]][[2]], 2)), 'x + ', as.character(round(model_20250121[[1]][[1]], 2)))

p0121 <- ggplot(benchmark_raw_1)+
  geom_point(aes(x=log_pM, y=Average.Ct, color=benchmark))+
  geom_abline(slope=model_20250121[[1]][[2]], intercept=model_20250121[[1]][[1]])+
  theme_classic()+
  labs(title='20250121')+
  annotate('text', label=text_formula, x=-0.5, y=21)
```

```{r}
#20250527
raw <- read.xlsx("C:\\Users\\chand\\Box Sync\\Krasileva_Lab\\Research\\chandler\\Krasileva Lab\\E19\\Library qPCR\\20250527.xlsx", sheet='Concentration Analysis', startRow=1, fillMergedCells = TRUE)  %>%
  mutate(Ct=as.double(Ct))

benchmark_raw <- raw %>% 
  filter(str_detect(sample, '^benchmark')) %>% 
  subset(select=c(sample, Average.Ct, dilution.factor)) %>% 
  unique() %>% 
  separate(sample, into=c(NA, 'benchmark', NA, 'replicate'), sep=' ') %>%
  mutate(ideal_nM = case_when(benchmark == '1' ~ 0.1, 
                              benchmark == '2' ~ 0.04, 
                              benchmark == '3' ~ 0.02, 
                              benchmark == '4' ~ 0.01, 
                              benchmark == '5' ~ 0.005, 
                              benchmark == '6' ~ 0.0095)) %>% 
  mutate(log_pM = log10(ideal_nM/dilution.factor*1000))

model_20250527 <- lm(Average.Ct~log_pM, benchmark_raw)
summary(model_20250527)

text_formula <- paste0("y = ", as.character(round(model_20250527[[1]][[2]], 2)), 'x + ', as.character(round(model_20250527[[1]][[1]], 2)))

p0527 <- ggplot(benchmark_raw)+
  geom_point(aes(x=log_pM, y=Average.Ct, color=benchmark))+
  geom_abline(slope=model_20250527[[1]][[2]], intercept=model_20250527[[1]][[1]])+
  labs(title='20250527')+
  annotate('text', label=text_formula, x=-0.5, y=21)+
  theme_classic()
```



```{r}
#20250527_2
raw_2 <- read.xlsx("C:\\Users\\chand\\Box Sync\\Krasileva_Lab\\Research\\chandler\\Krasileva Lab\\E19\\Library qPCR\\20250527_2.xlsx", sheet='Concentration Analysis', startRow=1, fillMergedCells = TRUE)  %>%
  mutate(Ct=as.double(Ct))

benchmark_raw_2 <- raw_2 %>% 
  filter(str_detect(sample, '^benchmark')) %>% 
  subset(select=c(sample, Average.Ct, dilution.factor)) %>% 
  unique() %>% 
  separate(sample, into=c(NA, 'benchmark', NA, 'replicate'), sep=' ') %>%
  mutate(ideal_nM = case_when(benchmark == '1' ~ 0.1, 
                              benchmark == '2' ~ 0.04, 
                              benchmark == '3' ~ 0.02, 
                              benchmark == '4' ~ 0.01, 
                              benchmark == '5' ~ 0.005, 
                              benchmark == '6' ~ 0.0095)) %>% 
  mutate(log_pM = log10(ideal_nM/dilution.factor*1000))



model_20250527_2 <- lm(Average.Ct~log_pM, benchmark_raw_2)
model_20250527_2[[1]][[2]]
summary(model_20250527_2)

text_formula <- paste0("y = ", as.character(round(model_20250527_2[[1]][[2]], 2)), 'x + ', as.character(round(model_20250527_2[[1]][[1]], 2)))

p0527_2 <- ggplot(benchmark_raw_2)+
  geom_point(aes(x=log_pM, y=Average.Ct, color=benchmark))+
  geom_abline(slope=model_20250527_2[[1]][[2]], intercept=model_20250527_2[[1]][[1]])+
  theme_classic()+
  labs(title='20250527_2')+
  annotate('text', label=text_formula, x=-0.5, y=21)
```

```{r}
p0121 + p0527 + p0527_2 + plot_layout(guides='collect')
```

Standard curve slopes between -3.1 and -3.6 are good (as shown here). Could adopt this as a new standard curve should I choose? Does that change the outcome of concentration? 

```{r}
stand_comp_1 <- raw %>% 
  filter(sample != 'ntc') %>% 
  filter(!str_detect(sample, '^bench')) %>% 
  subset(select=c(sample, Average.Ct, dilution.factor, fragment.size, `nM.(fmol/uL)`))%>%
  unique() %>%
  mutate(bench_calc_pmol = 10^((Average.Ct-model_20250527[[1]][[1]])/model_20250527[[1]][[2]])) %>%
  mutate(bench_calc_nM = bench_calc_pmol*dilution.factor*319/fragment.size/1000) %>%
  rename('standard_nM' = 'nM.(fmol/uL)') %>%
  subset(select=c(sample, standard_nM, bench_calc_nM))%>%
  mutate(date='20250527')

stand_comp_2 <- raw_2 %>% 
  filter(sample != 'ntc') %>% 
  filter(!str_detect(sample, '^bench')) %>% 
  subset(select=c(sample, Average.Ct, dilution.factor, fragment.size, `nM.(fmol/uL)`))%>%
  unique() %>%
  mutate(bench_calc_pmol = 10^((Average.Ct-model_20250527_2[[1]][[1]])/model_20250527_2[[1]][[2]])) %>%
  mutate(bench_calc_nM = bench_calc_pmol*dilution.factor*319/fragment.size/1000) %>%
  rename('standard_nM' = 'nM.(fmol/uL)') %>%
  subset(select=c(sample, standard_nM, bench_calc_nM)) %>%
  mutate(date='20250527_2')

stand_comp <- rbind(stand_comp_1, stand_comp_2)

s1 <- stand_comp %>%
  ggplot()+
  geom_point(aes(x=standard_nM, y=bench_calc_nM, color=date))+
  geom_abline(slope=1, intercept=0)+
  ylim(0, 0.3)+
  xlim(0, 0.3)+
  theme_classic() +
  ylab('benchmark nM')+
  xlab('standard nM')#+
 # scale_y_log10()+
#  scale_x_log10()

s2 <- stand_comp %>%
  ggplot()+
  geom_point(aes(x=standard_nM, y=bench_calc_nM, color=date))+
  geom_abline(slope=1, intercept=0)+
  theme_classic() +
  scale_y_log10()+
  scale_x_log10()+
  ylab('log10(benchmark nM)')+
  xlab('log10(standard nM)')

s1 + s2 + plot_layout(guides='collect')
```

```{r}
s_lib_comp <- stand_comp_1 %>% 
  filter(str_detect(sample, '^library')) %>%
  separate(sample, into=c(NA, 'library', NA, 'dilution'), sep=' ') %>%
  group_by(library) %>%
  summarize(standard_nM = mean(standard_nM), 
            bench_calc_nM = mean(bench_calc_nM)) %>%
  mutate(factor=standard_nM/bench_calc_nM) %>% 
  mutate(library=paste('library', library))

s_lib_comp_dilutions <- stand_comp_2 %>% 
  filter(str_detect(sample, '^library')) %>%
  separate(sample, into=c('library', 'lib_num', 'dilution', 'dil_number')) %>%
  mutate(library=paste(library, lib_num)) %>%
  mutate(dilution = paste(dilution, dil_number)) %>%
  mutate(Tube=case_when(dilution %in% c('dilution 1', 'dilution 4') ~ 'Tube 1', 
                        dilution %in% c('dilution 2', 'dilution 5') ~ 'Tube 2',
                        dilution %in% c('dilution 3', 'dilution 6') ~ 'Tube 3')) %>%
  subset(select=c(library, Tube, standard_nM, bench_calc_nM)) %>%
  mutate(factor=standard_nM/bench_calc_nM)
  

ggplot(s_lib_comp_dilutions)+
  geom_hline(yintercept=0.01, color='#00BA38')+
  geom_hline(yintercept=0.0075, color='#F8766D')+
  geom_hline(yintercept=0.0125, color='#619CFF')+
  geom_boxplot(aes(x=library, y=bench_calc_nM, color=Tube))+
  theme_classic()+
  ylim(0, 0.03)+
  xlab('')+
  ylab('working concentration (nM)')
```

Check linearity....

```{r}
s_lib_est <- s_lib_comp_dilutions  %>% group_by(library, Tube) %>%
  summarize(standard_nM=mean(standard_nM, na.rm = T),
            bench_calc_nM=mean(bench_calc_nM, na.rm=T), 
            factor=mean(factor, na.rm=T)) %>%
  arrange(Tube, library) %>% 
  add_column(df=c(13.76, 12.07, 14.87, 13.18, 11.38, 9.11, 9.40, 10.89, 4.47, 10.32, 9.05, 11.15, 9.88, 8.53, 6.83, 7.05, 8.17, 3.35, 8.25, 7.24, 8.92, 7.91, 6.83, 5.47, 5.64, 6.53, 2.68)) %>%
  mutate(standard_lib_est = standard_nM*df, 
         bench_lib_est = bench_calc_nM*df) %>%
  subset(select=c(library, Tube, standard_lib_est, bench_lib_est)) %>% 
  mutate(estimation = 'from_dilution')

slib <- s_lib_comp %>% 
  pivot_longer(names_to = 'standard', cols=c(standard_nM, bench_calc_nM)) %>% 
  subset(select=-factor) %>%
  mutate(date='20250527') %>%
  mutate(standard=case_when(standard=='standard_nM' ~ 'gblock', 
                            standard=='bench_calc_nM' ~ 'benchmark'))

sdil <- s_lib_est %>% 
  pivot_longer(names_to='standard', cols=c(standard_lib_est, bench_lib_est)) %>%
  mutate(standard=case_when(standard=='standard_lib_est' ~ 'gblock', 
                            standard=='bench_lib_est' ~ 'benchmark'))

ggplot()+
  geom_bar(slib, mapping=aes(x=library, y=value), stat='identity')+
  geom_point(sdil, mapping=aes(x=library, y=value))+
  theme_classic()+
  facet_wrap(~standard)

comparison <- sdil %>% 
  group_by(library, standard) %>% 
  summarize(value=mean(value)) %>% 
  mutate(date='20250527_2') %>%
  rbind(slib) %>%
  pivot_wider(names_from=date, values_from=value) %>%
  mutate(factor=`20250527_2`/`20250527`) 

comparison %>% 
  ggplot()+
  geom_point(aes(x=`20250527`, `20250527_2`, color=standard))+
  theme_classic()+
  geom_abline(intercept=0, slope=1)+
  xlim(0, 0.15)+
  ylim(0, 0.15)

comparison%>%
  group_by(standard)%>%
  summarize(factor=mean(factor))
```

For both benchmark and gblock, nonlinearity. Oh well! Continue with standards, as that is what everything is relative to, and technically smaller overestimation factor than benchmark. Very consistent with overestimation factor 